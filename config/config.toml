# Global LLM configuration
[llm]
api_type = 'ollama'
model = "qwen2.5-coder:latest"
base_url = "http://host.docker.internal:11434/v1"
api_key = "ollama"
max_tokens = 4096
temperature = 0.0

# Optional configuration for specific LLM models
[llm.vision]
api_type = 'ollama'
model = "qwen2.5-coder:latest"
base_url = "http://host.docker.internal:11434/v1"
api_key = "ollama"
max_tokens = 4096
temperature = 0.0

# Optional configuration, Search settings.
# [search]
# Search engine for agent to use. Default is "Google", can be set to "Baidu" or "DuckDuckGo".
engine = "Google"
# Fallback engine order. Default is ["DuckDuckGo", "Baidu"] - will try in this order after primary engine fails.
fallback_engines = ["DuckDuckGo", "Baidu"]
# Seconds to wait before retrying all engines again when they all fail due to rate limits. Default is 60.
retry_delay = 60
# Maximum number of times to retry all engines when all fail. Default is 3.
max_retries = 3


## Sandbox configuration
#[sandbox]
use_sandbox = false
image = "python:3.12-slim"
work_dir = "/workspace"
memory_limit = "1g"  # 512m
cpu_limit = 2.0
timeout = 300
network_enabled = true